Sys.setlocale("LC_ALL","en_US")
Sys.setlocale("LC_ALL","C")
from.dat <- as.Date("01/01/08", format("%m/%d/%y"))
to.dat <- as.Date("12/31/13", format("%m/%d/%y"))
getSymbols("GOOG", src="google", from=from.dat, to=to.dat)
head(GOOG)
library(quantmod)
from.dat <- as.Date("01/01/08", format("%m/%d/%y"))
to.dat <- as.Date("12/31/13", format("%m/%d/%y"))
getSymbols("GOOG", src="google", from=from.dat, to=to.dat)
head(GOOG)
from.dat <- as.Date("01/01/08", format("%m/%d/%y"))
to.dat <- as.Date("12/31/13", format("%m/%d/%y"))
getSymbols("GOOG", src="google", from=from.dat, to=to.dat)
head(GOOG)
from.dat <- as.Date("01/01/08", format("%m/%d/%y"))
to.dat <- as.Date("12/31/13", format("%m/%d/%y"))
getSymbols("GOOG", src="google", from=from.dat, to=to.dat)
head(GOOG)
from.dat <- as.Date("01/01/08", format("%m/%d/%y"))
to.dat <- as.Date("12/31/13", format("%m/%d/%y"))
getSymbols("GOOG", src="google", from=from.dat, to=to.dat)
head(GOOG)
from.dat <- as.Date("01/01/08", format("%m/%d/%y"))
to.dat <- as.Date("12/31/13", format("%m/%d/%y"))
getSymbols("GOOG", src="google", from=from.dat, to=to.dat)
head(GOOG)
from.dat <- as.Date("01/01/08", format("%m/%d/%y"))
to.dat <- as.Date("12/31/13", format("%m/%d/%y"))
getSymbols("GOOG", src="google", from=from.dat, to=to.dat)
head(GOOG)
from.dat <- as.Date("01/01/08", format("%m/%d/%y"))
to.dat <- as.Date("12/31/13", format("%m/%d/%y"))
getSymbols("GOOG", src="google", from=from.dat, to=to.dat)
head(GOOG)
getSymbols("GOOG", src="google", from=from.dat, to=to.dat)
from.dat <- as.Date("01/01/14", format("%m/%d/%y"))
to.dat <- as.Date("12/31/15", format("%m/%d/%y"))
getSymbols("GOOG", src="google", from=from.dat, to=to.dat)
head(GOOG)
invisible(Sys.setlocale("LC_MESSAGES", "C"))
Sys.setlocale("LC_ALL","")
Sys.setlocale("LC_ALL","")
Sys.setlocale("LC_ALL","English")
from.dat <- as.Date("01/01/14", format("%m/%d/%y"))
to.dat <- as.Date("12/31/15", format("%m/%d/%y"))
getSymbols("GOOG", src="google", from=from.dat, to=to.dat)
head(GOOG)
Sys.setlocale("LC_ALL","US")
from.dat <- as.Date("01/01/14", format("%m/%d/%y"))
to.dat <- as.Date("12/31/15", format("%m/%d/%y"))
getSymbols("GOOG", src="google", from=from.dat, to=to.dat)
head(GOOG)
data("iris")
library(ggplot2)
library(caret)
inTrain <- createDataPartition(iris$Species, p=0.7, list = FALSE)
training <- iris[inTrain,]
testing <- iris[-inTrain,]
dim(testin)
dim(testing)
kMeans1 <- kmeans(subset(training, select = -Species), centers = 3)
training$clasters <- as.factor(kMeans1$cluster)
qplot(Petal.Width, Petal.Length, data = training, col=clusters)
training$clasters <- as.factor(kMeans1$cluster)
qplot(Petal.Width, Petal.Length, data = training, col=clusters)
training$clusters <- as.factor(kMeans1$cluster)
qplot(Petal.Width, Petal.Length, data = training, col=clusters)
modFit <- train(clusters ~., data = training, method="rpart")
table(predict(modFit, training), training$Species)
modFit <- train(clusters ~.
, data = subset(training, select = -Species)
, method="rpart")
table(predict(modFit, training), training$Species)
modFit <- train(clusters ~.
, data = subset(training, select = -c(Species))
, method="rpart")
table(predict(modFit, training), training$Species)
table(predict(modFit, testing), testing$Species)
training$clusters <- as.factor(kMeans1$cluster)
modFit <- train(clusters ~.
, data = subset(training, select = -c(Species))
, method="rpart")
data("iris")
library(ggplot2)
library(caret)
inTrain <- createDataPartition(iris$Species, p=0.7, list = FALSE)
training <- iris[inTrain,]
testing <- iris[-inTrain,]
dim(testing)
kMeans1 <- kmeans(subset(training, select = -Species), centers = 3)
training$clusters <- as.factor(kMeans1$cluster)
qplot(Petal.Width, Petal.Length, data = training, col=clusters)
modFit <- train(clusters ~.
, data = subset(training, select = -c(Species))
, method="rpart")
table(predict(modFit, training), training$Species)
data("iris")
library(ggplot2)
library(caret)
inTrain <- createDataPartition(iris$Species, p=0.7, list = FALSE)
training <- iris[inTrain,]
testing <- iris[-inTrain,]
kMeans1 <- kmeans(subset(training, select = -Species), centers = 3)
training$clusters <- as.factor(kMeans1$cluster)
qplot(Petal.Width, Petal.Length, data = training, col=clusters)
modFit <- train(clusters ~.
, data = subset(training, select = -c(Species))
, method="rpart")
table(predict(modFit, training), training$Species)
table(predict(modFit, testing), testing$Species)
library(ElemStatLearn)
data(vowel.train)
data(vowel.test)
vowel.train$y <- as.factor(vowel.train$y)
vowel.test$y <- as.factor(vowel.test$y)
set.seed(33833)
library(caret)
mod1 <- train(y ~ ., data=vowel.train, method="rf")
mod2 <- train(y ~ ., data=vowel.train, method="gbm")
mod2 <- train(y ~ ., data=vowel.train, method="gbm")
pred1 <- predict(mod1, testing)
pred1 <- predict(mod1, vowel.test)
pred2 <- predict(mod2, vowel.test)
predDF <- data.frame(pred1, pred2, y=vowel.test$y)
combMod <- train(y ~ ., data = predDF, method="gam")
combPred <- predict(combMod, predDF)
sum(sqrt(pred1$fitting - vowel.test$y))
sum(sqrt(pred1 - vowel.test$y))
sqrt(sum((pred1-vowel.test$y)^2))
head(pred1)
qplot(pred1, pred2)
qplot(pred1, pred2, col=vowel.test$y)
?vowel.train
sqrt(sum((as.numeric(pred1)-as.numeric(vowel.test$y))^2))
?accuracy
table(pred1,vowel.test$y)
table(pred2,vowel.test$y)
table(combPred,vowel.test$y)
confusionMatrix(vowel.test$y, pred1)
confusionMatrix(vowel.test$y, pred2)
confusionMatrix(vowel.test$y, combPred)
agreed <- pred1 == pred2
confusionMatrix(vowel.test$y[agreed], pred1[agreed])
confusionMatrix(vowel.test$y[agreed], pred2[agreed])
library(gbm)
set.seed(3433)
library(AppliedPredictiveModeling)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
set.seed(62433)
str(training)
modRF <- train(diagnosis ~ ., data = training, method="rf")
modGBM <- train(diagnosis ~ ., data = training, method="gbm")
modLDA <- train(diagnosis ~ ., data = training, method="lda")
predRF <- predict(modRF, testing)
predGBM <- predict(modGBM, testing)
predLDA <- predict(modLDA, testing)
predDF <- data.frame(predRF, predGBM, predLDA, diagnosis=testing$diagnosis)
str(testing$diagnosis)
combMod <- train(diagnosis ~ ., data = predDF, method="rf")
confusionMatrix(testing$diagnosis, predRF)
combPred <- predict(combMod, testing)
confusionMatrix(testing$diagnosis, predRF)$overall['Accuracy']
confusionMatrix(testing$diagnosis, predGBM)$overall['Accuracy']
confusionMatrix(testing$diagnosis, predLDA)$overall['Accuracy']
confusionMatrix(testing$diagnosis, combPred)$overall['Accuracy']
## Q3
set.seed(3523)
library(AppliedPredictiveModeling)
data(concrete)
inTrain = createDataPartition(concrete$CompressiveStrength, p = 3/4)[[1]]
training = concrete[ inTrain,]
testing = concrete[-inTrain,]
set.seed(233)
str\
str(training)
modFit <- train(CompressiveStrength ~ ., data = training, method = "lasso")
modFit <- train(CompressiveStrength ~ ., data = training, method = "lasso")
modFit
?plot.enet
plot.enet(modFit)
modFit$finalModel
plot.enet(modFit$finalModel)
plot.enet(modFit$finalModel, xvar = "penalty", use.color = TRUE)
get.wd()
getwd()
library(lubridate) # For year() function below
dat = read.csv("gaData.csv")
training = dat[year(dat$date) < 2012,]
testing = dat[(year(dat$date)) > 2011,]
tstrain = ts(training$visitsTumblr)
install.packages("lubridate")
library(lubridate) # For year() function below
training = dat[year(dat$date) < 2012,]
testing = dat[(year(dat$date)) > 2011,]
tstrain = ts(training$visitsTumblr)
library(forecast)
install.packages("forecast")
library(forecast)
str(training)
summary(training$X)
training[x=="32",]
training[X=="32",]
training[training$X=="32",]
dat[dat$X=="32",]
dat[dat$X=="366",]
modFit <- bats(visitsTumblr ~ ., data=training)
?bats
modFit <- bats(training)
modFit <- bats(ts(training))
USAccDeaths
taylor
class(USAccDeaths)
ts(training)
modFit <- bats(ts(training[,-X]))
modFit <- bats(ts(training[,-"X"]))
modFit <- bats(ts(training[,-c("X")]))
modFit <- bats(ts(training[,-1]))
ts(training[,-1])
training$date <- as.Date(training$date)
modFit <- bats(ts(training[,-1]))
modFit <- bats(training[,-1])
training[,-1]
str(training)
class(training[,-1])
class(ts(training[,-1]))
class(as.ts(training[,-1]))
modFit <- bats(as.(training[,-1]))
modFit <- bats(as.ts(training[,-1]))
modFit <- bats(tstrain)
tstrain
forecast(modFit, testing)
forecast(modFit, testing$visitsTumblr)
forecast(modFit)
?forecast
forecast(modFit, h=365)
fcast <- forecast(modFit, h=365)
head(fcast$lower)
tail(fcast$lower)
tail(fcast$lower[,2])
mean(fcast$lower[,2] <= testing$visitsTumblr & fcast$upper[,2] >= testing$visitsTumblr)
length(fcast$lower[,2])
length(fcast$upper[,2])
length(testing$visitsTumblr)
fcast <- forecast(modFit, h=235)
mean(fcast$lower[,2] <= testing$visitsTumblr & fcast$upper[,2] >= testing$visitsTumblr)
set.seed(3523)
library(AppliedPredictiveModeling)
data(concrete)
inTrain = createDataPartition(concrete$CompressiveStrength, p = 3/4)[[1]]
training = concrete[ inTrain,]
testing = concrete[-inTrain,]
set.seed(325)
library(e1071)
?svm
str(training)
modFit <- svm(CompressiveStrength ~ ., data = training)
pred <- predict(modFit, testing)
sqrt(sum((pred - testing$CompressiveStrength)^2))
sqrt(mean((pred - testing$CompressiveStrength)^2))
myHist <- function(mu) {
mse <- mean((galton$child - mu)^2)
g <- ggplot(galton, aes(x=child))
g <- g + geom_histogram(fill="salmon", color="black", binwidth = 1)
g <- g + geom_vline(xintercept = mu, size=3)
g <- g + ggtitle(paste0("mu = ", mu, ", MSE = ", round(mse, 2)))
g
}
manipulate(myHist(mu), mu = slider(62, 74, step = .5))
library(ggplot2)
myHist <- function(mu) {
mse <- mean((galton$child - mu)^2)
g <- ggplot(galton, aes(x=child))
g <- g + geom_histogram(fill="salmon", color="black", binwidth = 1)
g <- g + geom_vline(xintercept = mu, size=3)
g <- g + ggtitle(paste0("mu = ", mu, ", MSE = ", round(mse, 2)))
g
}
manipulate(myHist(mu), mu = slider(62, 74, step = .5))
library(shiny)
library(manipulate)
myHist <- function(mu) {
mse <- mean((galton$child - mu)^2)
g <- ggplot(galton, aes(x=child))
g <- g + geom_histogram(fill="salmon", color="black", binwidth = 1)
g <- g + geom_vline(xintercept = mu, size=3)
g <- g + ggtitle(paste0("mu = ", mu, ", MSE = ", round(mse, 2)))
g
}
manipulate(myHist(mu), mu = slider(62, 74, step = .5))
data(galton)
library(HistData)
data(galton)
str(Galton)
data(Galton)
data(galton)
myHist <- function(mu) {
mse <- mean((galton$child - mu)^2)
g <- ggplot(galton, aes(x=child))
g <- g + geom_histogram(fill="salmon", color="black", binwidth = 1)
g <- g + geom_vline(xintercept = mu, size=3)
g <- g + ggtitle(paste0("mu = ", mu, ", MSE = ", round(mse, 2)))
g
}
manipulate(myHist(mu), mu = slider(62, 74, step = .5))
galton <- data(Galton)
myHist <- function(mu) {
mse <- mean((galton$child - mu)^2)
g <- ggplot(galton, aes(x=child))
g <- g + geom_histogram(fill="salmon", color="black", binwidth = 1)
g <- g + geom_vline(xintercept = mu, size=3)
g <- g + ggtitle(paste0("mu = ", mu, ", MSE = ", round(mse, 2)))
g
}
manipulate(myHist(mu), mu = slider(62, 74, step = .5))
galton <- Galton
myHist <- function(mu) {
mse <- mean((galton$child - mu)^2)
g <- ggplot(galton, aes(x=child))
g <- g + geom_histogram(fill="salmon", color="black", binwidth = 1)
g <- g + geom_vline(xintercept = mu, size=3)
g <- g + ggtitle(paste0("mu = ", mu, ", MSE = ", round(mse, 2)))
g
}
manipulate(myHist(mu), mu = slider(62, 74, step = .5))
plot(galton$parent,galton$child,pch=19,col="blue")
freqData <- as.data.frame(table(galton$child, galton$parent))
names(freqData) <- c("child", "parent", "freq")
plot(as.numeric(as.vector(freqData$parent)),
as.numeric(as.vector(freqData$child)),
# pch = 21, col = "black", bg = "lightblue",
cex = .15 * freqData$freq,
xlab = "parent", ylab = "child")
library(dplyr)
# constructs table for different combination of parent-child height
freqData <- as.data.frame(table(galton$child, galton$parent))
names(freqData) <- c("child (in)", "parent (in)", "freq")
# convert to numeric values
freqData$child <- as.numeric(as.character(freqData$child))
freqData$parent <- as.numeric(as.character(freqData$parent))
# filter to only meaningful combinations
g <- ggplot(filter(freqData, freq > 0), aes(x = parent, y = child))
g <- g + scale_size(range = c(2, 20), guide = "none" )
# plot grey circles slightly larger than data as base (achieve an outline effect)
g <- g + geom_point(colour="grey50", aes(size = freq+10, show_guide = FALSE))
# plot the accurate data points
g <- g + geom_point(aes(colour=freq, size = freq))
# change the color gradient from default to lightblue -> $white
g <- g + scale_colour_gradient(low = "lightblue", high="white")
g
g <- ggplot(filter(freqData, Freq>0), aes(x=parent, y=child))
g <- g + scale_size(range = c(2,20), guide = "none")
g <- g + geom_point(colour="grey50", aes(size = Freq+10, show_guide = FALSE))
g <- g + geom_point(aes(color=Freq, size=Freq))
g <- g + scale_colour_gradient(low = "lightblue", high="white")
g
library(dplyr)
# constructs table for different combination of parent-child height
freqData <- as.data.frame(table(galton$child, galton$parent))
names(freqData) <- c("child (in)", "parent (in)", "freq")
# convert to numeric values
freqData$child <- as.numeric(as.character(freqData$child))
freqData$parent <- as.numeric(as.character(freqData$parent))
# filter to only meaningful combinations
g <- ggplot(filter(freqData, freq > 0), aes(x = parent, y = child))
g <- g + scale_size(range = c(2, 20), guide = "none" )
# plot grey circles slightly larger than data as base (achieve an outline effect)
g <- g + geom_point(colour="grey50", aes(size = freq+10, show_guide = FALSE))
# plot the accurate data points
g <- g + geom_point(aes(colour=freq, size = freq))
# change the color gradient from default to lightblue -> $white
g <- g + scale_colour_gradient(low = "lightblue", high="white")
g
freqData
g <- ggplot(filter(freqData, freq>0), aes(x=parent, y=child))
g <- g + scale_size(range = c(2,20), guide = "none")
g <- g + geom_point(colour="grey50", aes(size = freq+10, show_guide = FALSE))
g <- g + geom_point(aes(color=freq, size=freq))
g <- g + scale_colour_gradient(low = "lightblue", high="white")
g
freqData <- as.data.frame(table(galton$child, galton$parent))
names(freqData) <- c("child", "parent", "freq")
plot(as.numeric(as.vector(freqData$parent)),
as.numeric(as.vector(freqData$child)),
pch = 21, col = "black", bg = "lightblue",
cex = .05 * freqData$freq,
xlab = "parent", ylab = "child", xlim = c(62, 74), ylim = c(62, 74))
abline(mean(y) - mean(x) * cor(y, x) * sd(y) / sd(x), sd(y) / sd(x) * cor(y, x), lwd = 3, col = "red")
abline(mean(y) - mean(x) * sd(y) / sd(x) / cor(y, x), sd(y) / sd(x) / cor(y, x), lwd = 3, col = "blue")
abline(mean(y) - mean(x) * sd(y) / sd(x), sd(y) / sd(x), lwd = 2)
points(mean(x), mean(y), cex = 2, pch = 19)
freqData <- as.data.frame(table(galton$child, galton$parent))
freqData
names
names(freqData)
lambda <- 0.2
n <- 40
nosim <- 1000
tmean <- 1/lambda
tsd <- 1/lambda
tvar <- tsd^2
set.seed(seed = 300)
sim <- rexp(n, lambda)
hist(sim, breaks = 10, col="wheat")
abline(v=mean(sim), col="green", lwd=2)
abline(v=tmean, col="red", lwd=2)
diffsd <- abs(sd(sim) - tsd)
diffmean <- abs(mean(sim) - tmean)
meansims <- sapply(1:nosim, function(i) mean(rexp(n, lambda)))
sdsims <- sapply(1:nosim, function(i) sd(rexp(n, lambda)))
hist(sims
, breaks = 30
, col="steelblue"
, main = "1000 Simulations of Exponential"
, xlab = "Means")
abline(v=mean(sims), col="green", lwd=2)
abline(v=tmean, col="red", lwd=2)
text(x=mean(sims)
, y=90
, col="green"
, labels = round(mean(sims),2)
, pos = 4)
text(x=tmean
, y=100
, col="red"
, labels = round(tmean,2)
, pos = 4)
diffsd <- abs(sd(sims)/sqrt(n) - tsd/sqrt(n))
diffmean <- abs(mean(sims) - tmean)
?rexp
source('L:/SkyDrive/Coursera/Data Science Specialization/SomePM2.R')
install.packages("shiny")
library(shiny)
library(shiny)
shinyUI(pageWithSidebar(
headerPanel("Data Science FTW!")
, sidebarPanel(
h3('Sidebar text')
)
, mainPanel(
h3('Main Panel text')
)
))
runApp()
runApp()
library(shiny)
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
library(UsingR)
data(galton)
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
install.packages("ShinyApps")
install.packages("shinyApps")
install.packages("shinyapps")
install.packages('rsconnect')
rsconnect::setAccountInfo(name='zimin', token='5EB8012C95B89A897E8B60BF9BAD6CBA', secret='Wah2MqfhF87kW6kEtXMeMpiwUoXnVccgZE47DJW3')
library(devtools)
devtools::install_github("rstudio/shinyapps")
install.packages("digest")
install.packages("digest")
devtools::install_github("rstudio/shinyapps")
library(shinyapps)
deployApp()
deployApp("C:/Users/Sergey/Documents/shinyapps")
deployApp()
library(shiny)
deployApp()
")
install.packages("rCharts")
install.packages("rCharts")
library(devtools)
install_github('ramnathv/rCharts')
data("airquality")
dTable(airquality, sPaginationType = "full_numbers")
library(rCharts)
dTable(airquality, sPaginationType = "full_numbers")
library(manipulate)
myPlot <- function(s) {
plot(cars$dist - mean(cars$dist), cars$speed - mean(cars$speed))
abline(0, s)
}
?manipulate
manipulate(myPlot(s), s = slider(0, 2, step = 0.1))
runApp()
runApp()
install_github('ramnathv/slidify')
library(devtools)
install_github('ramnathv/slidify')
install_github('ramnathv/slidifyLibraries')
library(slidify)
?author
dir.create("./SlidifyProj")
author(deckdir = "./SlidifyProj")
author("test")
slidify('index.rmd')
library(knitr)
browseURL("index.html")
slidify('index.rmd')
browseURL("index.html")
